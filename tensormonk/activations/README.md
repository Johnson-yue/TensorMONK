### activations
* Activations: A supporting function for convolution and linear layers. Available options (elu/lklu/[maxo](https://arxiv.org/pdf/1302.4389.pdf)/prelu/relu/relu6/rmxo(relu + maxo)/sigm/[squash](https://arxiv.org/pdf/1710.09829.pdf)/[swish](https://arxiv.org/pdf/1710.05941v1.pdf)/tanh)
